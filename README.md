# Deep Learning Specialization

This repository contains materials and lab assignments for the [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) by [Andrew Ng](https://www.andrewng.org/), organized into five comprehensive courses:

## Course 1: Neural Networks and Deep Learning
**[Go to Course 1 on Coursera](https://www.coursera.org/learn/neural-networks-deep-learning)**

### Week 1: Introduction to Deep Learning
- **Topics Covered:**
  - What is a neural network?
  - Supervised learning with neural networks
  - Why is deep learning taking off?
- **Heroes of Deep Learning:** [Geoffrey Hinton](https://youtu.be/-eyhCTvrEtE)

### Week 2: Neural Networks Basics
- **Topics Covered:**
  - Binary classification
  - Logistic regression
  - Gradient descent
  - Python and vectorization
- **Heroes of Deep Learning:** [Pieter Abbeel](https://youtu.be/dmkPJpWCVcI)

### Week 3: Shallow Neural Networks
- **Topics Covered:**
  - Neural network representation
  - Computing neural network output
  - Vectorizing across multiple examples
  - Activation functions
- **Heroes of Deep Learning:** [Ian Goodfellow](https://youtu.be/pWAc9B2zJS4)

### Week 4: Deep Neural Networks
- **Topics Covered:**
  - Deep L-layer neural network
  - Forward and backward propagation
  - Building blocks of deep neural networks
  - Parameters vs hyperparameters

## Course 2: Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization
**[Go to Course 2 on Coursera](https://www.coursera.org/learn/deep-neural-network)**

### Week 1: Practical Aspects of Deep Learning
- **Topics Covered:**
  - Setting up your Machine Learning Application
  - Regularization techniques (L2, Dropout, Data Augmentation, Early Stopping)
  - Setting up your optimization problem
  - Normalizing inputs
  - Vanishing/Exploding gradients
  - Weight initialization
  - Gradient checking
- **Heroes of Deep Learning:** [Yoshua Bengio](https://youtu.be/pnTLZQhFpaE)

### Week 2: Optimization Algorithms
- **Topics Covered:**
  - Mini-batch gradient descent
  - Exponentially weighted averages
  - Momentum and RMSprop
  - Adam optimization algorithm
  - Learning rate decay
  - The problem of local optima
- **Heroes of Deep Learning:** [Yuanqing Lin](https://youtu.be/3GfOnI3goAk)

### Week 3: Hyperparameter Tuning, Batch Normalization and Programming Frameworks
- **Topics Covered:**
  - Hyperparameter tuning
  - Batch normalization
  - Multi-class classification
  - Programming frameworks (TensorFlow, PyTorch)
  - TensorFlow implementation

## Course 3: Structuring your Machine Learning Project
**[Go to Course 3 on Coursera](https://www.coursera.org/learn/machine-learning-projects)**

## Course 4: Convolutional Neural Networks
**[Go to Course 4 on Coursera](https://www.coursera.org/learn/convolutional-neural-networks)**

## Course 5: Natural Language Processing: Building Sequence Models
**[Go to Course 5 on Coursera](https://www.coursera.org/learn/nlp-sequence-models)**

---

## Getting Started
1. Clone this repository.
2. Open the desired course folder and navigate to the lab assignments.
3. Launch the Jupyter Notebooks to begin working through the labs.

## Requirements
- Python 3.x
- Jupyter Notebook
- Common DL libraries: numpy, matplotlib, tensorflow, keras, etc.

## License
This repository is for educational purposes only. 
